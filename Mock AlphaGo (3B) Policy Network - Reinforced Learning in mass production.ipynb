{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock AlphaGo (3B) Policy Network - Reinforced Learning in mass production\n",
    "In this notebook, we will train the policy network by letting them compete each other according to DeepMind:\n",
    "\n",
    "> We further trained the policy network by policy gradient reinforcement learning.\n",
    "Each iteration consisted of a mini-batch of n games played in parallel, between\n",
    "the current policy network $p_\\rho$ that is being trained, and an opponent $p_\\rho-$\n",
    "that uses parameters $\\rho^-$ from a previous iteration, randomly sampled from\n",
    "a pool $O$ of opponents, so as to increase the stability of training. Weights were\n",
    "initialized to $\\rho = \\rho^- = \\sigma$. Every 500 iterations, we added the current\n",
    "parameters $\\rho$ to the opponent pool. Each game $i$ in the mini-batch was played\n",
    "out until termination at step $T^i$, and then scored to determine the outcome\n",
    "$z^i_t = \\pm r(s_{T^i})$ from each playerâ€™s perspective. The games were then replayed\n",
    "to determine the policy gradient update, $\\Delta\\rho = \\frac{a}{n}\\Sigma^n_{i=1}\n",
    "\\Sigma^{T^i}_{t=1}\\frac{\\partial\\log p_\\rho(a^i_t|s^i_t)}{\\partial_\\rho}(z^i_t-v(s^i_t))$, using the REINFORCE \n",
    "algorithm with baseline $v(s^i_t)$ for variance reduction. On the first pass \n",
    "through the training pipeline, the baseline was set to zero; on the second pass\n",
    "we used the value network $v_\\theta(s)$ as a baseline; this provided a small\n",
    "performance boost. The policy network was trained in this way for 10,000 \n",
    "mini-batches of 128 games, using 50 GPUs, for one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named caffe2_pybind11_state_gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in CPU mode\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from caffe2.python import core, model_helper, workspace, brew, utils\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from sgfutil import BOARD_POSITION\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# how many games will be run in one minibatch\n",
    "GAMES_BATCHES = 16 # [1,infinity) depends on your hardware\n",
    "# how many iterations for this tournament\n",
    "TOURNAMENT_ITERS = 1000 # [1,infinity)\n",
    "\n",
    "if workspace.has_gpu_support:\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CUDA, workspace.GetDefaultGPUID())\n",
    "    print('Running in GPU mode on default device {}'.format(workspace.GetDefaultGPUID()))\n",
    "else :\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CPU, 0)\n",
    "    print('Running in CPU mode')\n",
    "\n",
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "\n",
    "ROOT_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','go','param') # folder stores the loss/accuracy log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to differentiate primary player and sparring partner. Primary player will learn from the game result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-128-1(black) vs. 4-128-1(white)\n"
     ]
    }
   ],
   "source": [
    "# who is primary and who is sparring partner? \n",
    "PRIMARY_PLAYER = \"black\" # or white\n",
    "SPARRING_PLAYER = \"white\"\n",
    "\n",
    "### Config for primary player\n",
    "PRIMARY_WORKSPACE = os.path.join(ROOT_FOLDER, PRIMARY_PLAYER)\n",
    "PRIMARY_CONV_LEVEL = 4\n",
    "PRIMARY_FILTERS = 128\n",
    "PRIMARY_PRE_TRAINED_ITERS = 1\n",
    "# before traning, where to load the params\n",
    "PRIMARY_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-conv={}-k={}-iter={}\".format(PRIMARY_CONV_LEVEL,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS))\n",
    "\n",
    "### Config for sparring partner\n",
    "SPARR_WORKSPACE = os.path.join(ROOT_FOLDER, SPARRING_PLAYER)\n",
    "SPARR_CONV_LEVEL = 4\n",
    "SPARR_FILTERS = 128\n",
    "SPARR_PRE_TRAINED_ITERS = 1\n",
    "# before traning, where to load the params\n",
    "SPARR_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-conv={}-k={}-iter={}\".format(SPARR_CONV_LEVEL,SPARR_FILTERS,SPARR_PRE_TRAINED_ITERS))\n",
    "\n",
    "print('{}-{}-{}({}) vs. {}-{}-{}({})'.format(\n",
    "    PRIMARY_CONV_LEVEL, PRIMARY_FILTERS, PRIMARY_PRE_TRAINED_ITERS, PRIMARY_PLAYER,\n",
    "    SPARR_CONV_LEVEL, SPARR_FILTERS, SPARR_PRE_TRAINED_ITERS, SPARRING_PLAYER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following training parameters are only for primary player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training, result will be saved to /home/wangd/python/tutorial_data/go/param/RL-conv=4-k=128-iter=1001\n"
     ]
    }
   ],
   "source": [
    "BASE_LR = -0.003 # (-0.003,0) The base Learning Rate\n",
    "\n",
    "TRAIN_BATCHES = 16 # how many samples will be trained within one mini-batch, depends on your hardware\n",
    "\n",
    "# after training, where to store the params\n",
    "SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"RL-conv={}-k={}-iter={}\".\n",
    "                           format(PRIMARY_CONV_LEVEL,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))\n",
    "print('After training, result will be saved to {}'.format(SAVE_FOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modeling import AddConvModel, AddTrainingOperators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the actual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe2.python.predictor.predictor_exporter as pe\n",
    "\n",
    "data = np.empty(shape=(TRAIN_BATCHES,48,19,19), dtype=np.float32)\n",
    "label = np.empty(shape=(TRAIN_BATCHES,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary player\n",
    ">Train Net: Blob('data','label') ==> Predict Net ==> Loss ==> Backward Propergation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n"
     ]
    }
   ],
   "source": [
    "workspace.SwitchWorkspace(PRIMARY_WORKSPACE, True)\n",
    "# for learning from winner\n",
    "with core.DeviceScope(device_opts):\n",
    "    primary_train_model = model_helper.ModelHelper(name=\"primary_train_model\", arg_scope=arg_scope, init_params=True)\n",
    "    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "    predict = AddConvModel(primary_train_model, \"data\", conv_level=PRIMARY_CONV_LEVEL, filters=PRIMARY_FILTERS)\n",
    "    workspace.FeedBlob(\"label\", data, device_option=device_opts)\n",
    "    AddTrainingOperators(primary_train_model, predict, \"label\", base_lr=BASE_LR)\n",
    "    workspace.RunNetOnce(primary_train_model.param_init_net)\n",
    "    workspace.CreateNet(primary_train_model.net, overwrite=True)\n",
    "# for learning from negative examples\n",
    "with core.DeviceScope(device_opts):\n",
    "    primary_train_neg_model = model_helper.ModelHelper(name=\"primary_train_neg_model\", arg_scope=arg_scope, init_params=True)\n",
    "    #workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "    predict = AddConvModel(primary_train_neg_model, \"data\", conv_level=PRIMARY_CONV_LEVEL, filters=PRIMARY_FILTERS)\n",
    "    #workspace.FeedBlob(\"label\", data, device_option=device_opts)\n",
    "    AddTrainingOperators(primary_train_neg_model, predict, \"label\", base_lr=BASE_LR, learn_neg=True)\n",
    "    workspace.RunNetOnce(primary_train_neg_model.param_init_net)\n",
    "    workspace.CreateNet(primary_train_neg_model.net, overwrite=True)\n",
    "    \n",
    "primary_predict_net = pe.prepare_prediction_net(os.path.join(PRIMARY_LOAD_FOLDER, \"policy_model.minidb\"),\n",
    "                                               \"minidb\", device_option=device_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparring partner\n",
    ">Predict Net: Blob('data') ==> Predict Net ==> Blob('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sparring partner\n",
    "workspace.SwitchWorkspace(SPARR_WORKSPACE, True)\n",
    "sparring_predict_net = pe.prepare_prediction_net(os.path.join(SPARR_LOAD_FOLDER, \"policy_model.minidb\"),\n",
    "                                                 \"minidb\", device_option=device_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tournament and training\n",
    ">We use a reward function $r(s)$ that is zero for all non-terminal time-steps $t < T$.\n",
    "The outcome $z_t = \\pm r(s_T)$ is the terminal reward at the end of the game from the perspective of the\n",
    "current player at time-step $t$: $+1$ for winning and $-1$ for losing. Weights are then updated at each\n",
    "time-step $t$ by stochastic gradient ascent in the direction that maximizes expected outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tournament 0 Finished with Black(Primary) 11:5 White(Sparring)\n",
      "Tournament 1 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 2 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 3 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 4 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 5 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 6 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 7 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 8 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 9 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 10 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 11 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 12 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 13 Finished with Black(Primary) 0:16 White(Sparring)\n",
      "Tournament 14 Finished with Black(Primary) 0:16 White(Sparring)\n"
     ]
    }
   ],
   "source": [
    "from go import GameState, BLACK, WHITE, EMPTY, PASS\n",
    "from preprocessing import Preprocess\n",
    "from game import DEFAULT_FEATURES\n",
    "from datetime import datetime\n",
    "from sgfutil import GetWinner, WriteBackSGF\n",
    "import sgf\n",
    "\n",
    "np.random.seed(datetime.now().microsecond)\n",
    "\n",
    "# construct the model to be exported\n",
    "pe_meta = pe.PredictorExportMeta(\n",
    "    predict_net=primary_train_model.net.Proto(),\n",
    "    parameters=[str(b) for b in primary_train_model.params], \n",
    "    inputs=[\"data\"],\n",
    "    outputs=[\"predict\"],\n",
    ")\n",
    "\n",
    "for tournament in range(TOURNAMENT_ITERS):\n",
    "    # Every 500 tournament, copy current player to opponent. i.e. checkpoint\n",
    "    if tournament > 0 and tournament % 50 == 0:\n",
    "        pe.save_to_db(\"minidb\", os.path.join(SAVE_FOLDER, \"policy_model_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+tournament)), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(SAVE_FOLDER))\n",
    "        \n",
    "    # TODO: randomly pickup opponent\n",
    "    # TODO: randomly change color of player\n",
    "    game_state = [ GameState() for i in range(GAMES_BATCHES) ]\n",
    "    game_result = [0] * GAMES_BATCHES # 0 - Not Ended; BLACK - Black Wins; WHITE - White Wins\n",
    "    p = [ Preprocess(DEFAULT_FEATURES) ] * GAMES_BATCHES\n",
    "    history = [ [] for i in range(GAMES_BATCHES) ]\n",
    "    board = None\n",
    "    \n",
    "    if PRIMARY_PLAYER == 'black':\n",
    "        BLACK_PLAYER = 'Primary'\n",
    "        WHITE_PLAYER = 'Sparring'\n",
    "    else:\n",
    "        BLACK_PLAYER = 'Sparring'\n",
    "        WHITE_PLAYER = 'Primary'\n",
    "\n",
    "    # for each step in all games\n",
    "    for step in range(0,500):\n",
    "        board = np.concatenate([p[i].state_to_tensor(game_state[i]).astype(np.float32) for i in range(GAMES_BATCHES)])\n",
    "\n",
    "        if step % 2 == 0:\n",
    "            current_player = BLACK\n",
    "            current_color = 'B'\n",
    "        else:\n",
    "            current_player = WHITE\n",
    "            current_color = 'W'\n",
    "\n",
    "        if step % 2 == (PRIMARY_PLAYER == 'white'):\n",
    "            # primary player move\n",
    "            workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "            workspace.FeedBlob('data', board, device_option=device_opts)\n",
    "            workspace.RunNet(primary_predict_net)\n",
    "        else:\n",
    "            # sparring partner move\n",
    "            workspace.SwitchWorkspace(SPARR_WORKSPACE)\n",
    "            workspace.FeedBlob('data', board, device_option=device_opts)\n",
    "            workspace.RunNet(sparring_predict_net)\n",
    "\n",
    "        predict = workspace.FetchBlob('predict') # [0.01, 0.02, ...] in shape (N,361)\n",
    "\n",
    "        for i in range(GAMES_BATCHES):\n",
    "            if game_result[i]: # game end\n",
    "                continue\n",
    "            else: # game not end\n",
    "                legal_moves = [ x*19+y for (x,y) in game_state[i].get_legal_moves(include_eyes=False)] # [59, 72, ...] in 1D\n",
    "                if len(legal_moves) > 0: # at least 1 legal move\n",
    "                    probabilities = predict[i][legal_moves] # [0.02, 0.01, ...]\n",
    "                    # use numpy.random.choice to randomize the step,\n",
    "                    # otherwise use np.argmax to get best choice\n",
    "                    # current_choice = legal_moves[np.argmax(probabilities)]\n",
    "                    if np.sum(probabilities) > 0:\n",
    "                        current_choice = np.random.choice(legal_moves, 1, p=probabilities/np.sum(probabilities))[0]\n",
    "                    else:\n",
    "                        current_choice = np.random.choice(legal_moves, 1)[0]\n",
    "                    (x, y) = (current_choice/19, current_choice%19)\n",
    "                    history[i].append((current_color, x, y, board[i]))\n",
    "                    game_state[i].do_move(action = (x, y), color = current_player) # End of Game?\n",
    "                    #print('game({}) step({}) {} move({},{})'.format(i, step, current_color, x, y))\n",
    "                else:\n",
    "                    game_state[i].do_move(action = PASS, color = current_player)\n",
    "                    #print('game({}) step({}) {} PASS'.format(i, step, current_color))\n",
    "                    game_result[i] = game_state[i].is_end_of_game\n",
    "\n",
    "        if np.all(game_result):\n",
    "            break\n",
    "    \n",
    "    # Get the winner\n",
    "    winner = [ GetWinner(game_state[i]) for i in range(GAMES_BATCHES) ] # B+, W+, T\n",
    "    print('Tournament {} Finished with Black({}) {}:{} White({})'.\n",
    "          format(tournament, BLACK_PLAYER, sum(np.char.count(winner, 'B+')),\n",
    "                 sum(np.char.count(winner, 'W+')), WHITE_PLAYER)) \n",
    "    \n",
    "    # Save the games\n",
    "    for i in range(GAMES_BATCHES):\n",
    "        filename = os.path.join(\n",
    "            os.path.expanduser('~'), 'python', 'tutorial_files','selfplay',\n",
    "            '({}_{}_{})vs({}_{}_{})_{}_{}_{}'.format(PRIMARY_CONV_LEVEL, PRIMARY_FILTERS, PRIMARY_PRE_TRAINED_ITERS+tournament,\n",
    "                                            SPARR_CONV_LEVEL, SPARR_FILTERS, SPARR_PRE_TRAINED_ITERS, i, winner[i],\n",
    "                                            datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")))\n",
    "        #print(filename)\n",
    "        WriteBackSGF(winner, history[i], filename)\n",
    "    \n",
    "    # After each tournament, learn from the winner\n",
    "    #iter = 0\n",
    "    k = 0\n",
    "    for i in range(GAMES_BATCHES):\n",
    "        #print('Learning {} steps in {} of {} games'.format(iter * TRAIN_BATCHES, i, GAMES_BATCHES))\n",
    "        for step in history[i]:\n",
    "            if (step[0] == 'B' and winner[i] == 'B+') or (step[0] == 'W' and winner[i] == 'W+'):\n",
    "                data[k] = step[3]\n",
    "                label[k] = step[1]*19+step[2]\n",
    "                k += 1\n",
    "                if k == TRAIN_BATCHES:\n",
    "                    #iter += 1\n",
    "                    k = 0\n",
    "                    workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "                    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "                    workspace.FeedBlob(\"label\", label, device_option=device_opts)\n",
    "                    workspace.RunNet(primary_train_model.net)\n",
    "    \n",
    "    # And learn from negative examples\n",
    "    #iter = 0\n",
    "    k = 0\n",
    "    for i in range(GAMES_BATCHES):\n",
    "        #print('Learning negative examples {} steps in {} of {} games'.format(iter * TRAIN_BATCHES, i, GAMES_BATCHES))\n",
    "        for step in history[i]:\n",
    "            if (step[0] == 'B' and winner[i] == 'W+') or (step[0] == 'W' and winner[i] == 'B+'):\n",
    "                data[k] = step[3]\n",
    "                label[k] = step[1]*19+step[2]\n",
    "                k += 1\n",
    "                if k == TRAIN_BATCHES:\n",
    "                    #iter += 1\n",
    "                    k = 0\n",
    "                    workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "                    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "                    workspace.FeedBlob(\"label\", label, device_option=device_opts)\n",
    "                    workspace.RunNet(primary_train_neg_model.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "315px",
    "width": "367px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "544px",
    "left": "0px",
    "right": "1723px",
    "top": "107px",
    "width": "130px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
