{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock AlphaGo (3B) Policy Network - Reinforced Learning in mass production\n",
    "In this notebook, we will train the policy network by letting them compete each other according to DeepMind:\n",
    "\n",
    "> We further trained the policy network by policy gradient reinforcement learning.\n",
    "Each iteration consisted of a mini-batch of n games played in parallel, between\n",
    "the current policy network $p_\\rho$ that is being trained, and an opponent $p_\\rho-$\n",
    "that uses parameters $\\rho^-$ from a previous iteration, randomly sampled from\n",
    "a pool $O$ of opponents, so as to increase the stability of training. Weights were\n",
    "initialized to $\\rho = \\rho^- = \\sigma$. Every 500 iterations, we added the current\n",
    "parameters $\\rho$ to the opponent pool. Each game $i$ in the mini-batch was played\n",
    "out until termination at step $T^i$, and then scored to determine the outcome\n",
    "$z^i_t = \\pm r(s_{T^i})$ from each playerâ€™s perspective. The games were then replayed\n",
    "to determine the policy gradient update, $\\Delta\\rho = \\frac{a}{n}\\Sigma^n_{i=1}\n",
    "\\Sigma^{T^i}_{t=1}\\frac{\\partial\\log p_\\rho(a^i_t|s^i_t)}{\\partial_\\rho}(z^i_t-v(s^i_t))$, using the REINFORCE \n",
    "algorithm with baseline $v(s^i_t)$ for variance reduction. On the first pass \n",
    "through the training pipeline, the baseline was set to zero; on the second pass\n",
    "we used the value network $v_\\theta(s)$ as a baseline; this provided a small\n",
    "performance boost. The policy network was trained in this way for 10,000 \n",
    "mini-batches of 128 games, using 50 GPUs, for one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named caffe2_pybind11_state_gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in CPU mode\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from caffe2.python import core, model_helper, workspace, brew, utils\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from sgfutil import BOARD_POSITION\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# how many games will be run in one minibatch\n",
    "GAMES_BATCHES = 16 # [1,infinity) depends on your hardware\n",
    "# how many iterations for this tournament\n",
    "TOURNAMENT_ITERS = 10000 # [1,infinity)\n",
    "\n",
    "if workspace.has_gpu_support:\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CUDA, workspace.GetDefaultGPUID())\n",
    "    print('Running in GPU mode on default device {}'.format(workspace.GetDefaultGPUID()))\n",
    "else :\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CPU, 0)\n",
    "    print('Running in CPU mode')\n",
    "\n",
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "\n",
    "ROOT_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','go','param') # folder stores the loss/accuracy log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to differentiate primary player and sparring partner. Primary player will learn from the game result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from 1001 to 11001 iterations\n"
     ]
    }
   ],
   "source": [
    "### Config for primary player\n",
    "PRIMARY_WORKSPACE = os.path.join(ROOT_FOLDER, 'primary')\n",
    "PRIMARY_CONV_LEVEL = 4\n",
    "PRIMARY_FILTERS = 128\n",
    "PRIMARY_PRE_TRAINED_ITERS = 1001\n",
    "# before traning, where to load the params\n",
    "PRIMARY_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-conv={}-k={}-iter={}\"\n",
    "                                   .format(PRIMARY_CONV_LEVEL,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS))\n",
    "BASE_LR = -0.01 # (-0.003,0) The base Learning Rate; 0 to disable it.\n",
    "NEGATIVE_BASE_LR = 0.0 # [BASE_LR,0] Dues to multi-class softmax, this param is usually smaller than BASE_LR; 0 to disable it.\n",
    "TRAIN_BATCHES = 16 # how many samples will be trained within one mini-batch, depends on your hardware\n",
    "# after training, where to store the params\n",
    "PRIMARY_SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"RL-conv={}-k={}-iter={}\"\n",
    "                           .format(PRIMARY_CONV_LEVEL,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))\n",
    "if not os.path.exists(PRIMARY_SAVE_FOLDER):\n",
    "    os.makedirs(PRIMARY_SAVE_FOLDER)\n",
    "\n",
    "### Config for sparring partner\n",
    "SPARR_WORKSPACE = os.path.join(ROOT_FOLDER, 'sparring')\n",
    "SPARR_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"conv={}-k={}-iter={}\".format(4,128,1))\n",
    "\n",
    "print('Training model from {} to {} iterations'.format(PRIMARY_PRE_TRAINED_ITERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modeling import AddConvModel, AddTrainingOperators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the actual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe2.python.predictor.predictor_exporter as pe\n",
    "\n",
    "data = np.empty(shape=(TRAIN_BATCHES,48,19,19), dtype=np.float32)\n",
    "label = np.empty(shape=(TRAIN_BATCHES,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary player\n",
    ">Train Net: Blob('data','label') ==> Predict Net ==> Loss ==> Backward Propergation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Sub.\n"
     ]
    }
   ],
   "source": [
    "workspace.SwitchWorkspace(PRIMARY_WORKSPACE, True)\n",
    "# for learning from winner\n",
    "with core.DeviceScope(device_opts):\n",
    "    primary_train_model = model_helper.ModelHelper(name=\"primary_train_model\", arg_scope=arg_scope, init_params=True)\n",
    "    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "    predict = AddConvModel(primary_train_model, \"data\", conv_level=PRIMARY_CONV_LEVEL, filters=PRIMARY_FILTERS)\n",
    "    workspace.FeedBlob(\"label\", data, device_option=device_opts)\n",
    "    AddTrainingOperators(primary_train_model, predict, \"label\", base_lr=BASE_LR)\n",
    "    workspace.RunNetOnce(primary_train_model.param_init_net)\n",
    "    workspace.CreateNet(primary_train_model.net, overwrite=True)\n",
    "# for learning from negative examples\n",
    "with core.DeviceScope(device_opts):\n",
    "    primary_train_neg_model = model_helper.ModelHelper(name=\"primary_train_neg_model\", arg_scope=arg_scope, init_params=True)\n",
    "    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "    predict = AddConvModel(primary_train_neg_model, \"data\", conv_level=PRIMARY_CONV_LEVEL, filters=PRIMARY_FILTERS)\n",
    "    ONES = primary_train_neg_model.ConstantFill([], \"ONES\", shape=[TRAIN_BATCHES,361], value=1.0)\n",
    "    negative = primary_train_neg_model.Sub([ONES, predict], 'negative')\n",
    "    workspace.FeedBlob(\"label\", data, device_option=device_opts)\n",
    "    AddTrainingOperators(primary_train_neg_model, negative, \"label\", base_lr=NEGATIVE_BASE_LR)\n",
    "    workspace.RunNetOnce(primary_train_neg_model.param_init_net)\n",
    "    workspace.CreateNet(primary_train_neg_model.net, overwrite=True)\n",
    "    \n",
    "primary_predict_net = pe.prepare_prediction_net(os.path.join(PRIMARY_LOAD_FOLDER, \"policy_model.minidb\"),\n",
    "                                               \"minidb\", device_option=device_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `LearnFromWinner` takes the result of tournament and train primary player with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LearnFromWinningGames(history, winner, mini_batch=TRAIN_BATCHES, device_opts=device_opts):\n",
    "    data = np.empty(shape=(mini_batch,48,19,19), dtype=np.float32)\n",
    "    label = np.empty(shape=(mini_batch,), dtype=np.int32)\n",
    "    #iter = 0\n",
    "    k = 0\n",
    "    for i in range(len(winner)):\n",
    "        #print('Learning {} steps in {} of {} games'.format(iter * TRAIN_BATCHES, i, GAMES_BATCHES))\n",
    "        for step in history[i]:\n",
    "            if (step[0] == 'B' and winner[i] == 'B+') or (step[0] == 'W' and winner[i] == 'W+'):\n",
    "                data[k] = step[2]\n",
    "                label[k] = step[1]\n",
    "                k += 1\n",
    "                #iter += 1\n",
    "                if k == mini_batch:\n",
    "                    k = 0\n",
    "                    workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "                    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "                    workspace.FeedBlob(\"label\", label, device_option=device_opts)\n",
    "                    workspace.RunNet(primary_train_model.net)\n",
    "\n",
    "def LearnFromLosingGames(history, winner, mini_batch=TRAIN_BATCHES, device_opts=device_opts):\n",
    "    data = np.empty(shape=(mini_batch,48,19,19), dtype=np.float32)\n",
    "    label = np.empty(shape=(mini_batch,), dtype=np.int32)\n",
    "    #iter = 0\n",
    "    k = 0\n",
    "    for i in range(len(winner)):\n",
    "        #print('Learning {} steps in {} of {} games'.format(iter * TRAIN_BATCHES, i, GAMES_BATCHES))\n",
    "        for step in history[i]:\n",
    "            if (step[0] == 'B' and winner[i] == 'W+') or (step[0] == 'W' and winner[i] == 'B+'):\n",
    "                data[k] = step[2]\n",
    "                label[k] = step[1]\n",
    "                k += 1\n",
    "                #iter += 1\n",
    "                if k == mini_batch:\n",
    "                    k = 0\n",
    "                    workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "                    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "                    workspace.FeedBlob(\"label\", label, device_option=device_opts)\n",
    "                    workspace.RunNet(primary_train_neg_model.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparring partner\n",
    ">Predict Net: Blob('data') ==> Predict Net ==> Blob('predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tournament and training\n",
    ">We use a reward function $r(s)$ that is zero for all non-terminal time-steps $t < T$.\n",
    "The outcome $z_t = \\pm r(s_T)$ is the terminal reward at the end of the game from the perspective of the\n",
    "current player at time-step $t$: $+1$ for winning and $-1$ for losing. Weights are then updated at each\n",
    "time-step $t$ by stochastic gradient ascent in the direction that maximizes expected outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from go import GameState, BLACK, WHITE, EMPTY, PASS\n",
    "from preprocessing import Preprocess\n",
    "from game import DEFAULT_FEATURES\n",
    "from datetime import datetime\n",
    "from sgfutil import GetWinner, WriteBackSGF\n",
    "import sgf\n",
    "\n",
    "np.random.seed(datetime.now().microsecond)\n",
    "\n",
    "# construct the model to be exported\n",
    "pe_meta = pe.PredictorExportMeta(\n",
    "    predict_net=primary_predict_net.Proto(),\n",
    "    parameters=[str(b) for b in primary_train_model.params],\n",
    "    inputs=[\"data\"],\n",
    "    outputs=[\"predict\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tournament 1001 Primary(B) vs Sparring(W|policy_model_RL_41.minidb) started @2017-10-16 10:04:47.179973\n",
      "Tournament 1001 Finished with Primary(B) 6:10 Sparring(W) @2017-10-16 10:07:10.285108\n",
      "Tournament 1002 Primary(B) vs Sparring(W|policy_model.minidb) started @2017-10-16 10:10:11.489748\n",
      "Tournament 1002 Finished with Primary(B) 11:5 Sparring(W) @2017-10-16 10:12:31.591594\n",
      "Tournament 1003 Primary(W) vs Sparring(B|policy_model.minidb) started @2017-10-16 10:15:33.133998\n",
      "Tournament 1003 Finished with Primary(W) 16:0 Sparring(B) @2017-10-16 10:17:48.668242\n",
      "Tournament 1004 Primary(B) vs Sparring(W|policy_model_RL_161.minidb) started @2017-10-16 10:20:43.932180\n",
      "Tournament 1004 Finished with Primary(B) 4:12 Sparring(W) @2017-10-16 10:22:59.254353\n",
      "Tournament 1005 Primary(W) vs Sparring(B|policy_model_RL_141.minidb) started @2017-10-16 10:25:57.883861\n",
      "Tournament 1005 Finished with Primary(W) 7:9 Sparring(B) @2017-10-16 10:28:19.184951\n",
      "Tournament 1006 Primary(W) vs Sparring(B|policy_model_RL_101.minidb) started @2017-10-16 10:31:18.129769\n",
      "Tournament 1006 Finished with Primary(W) 10:6 Sparring(B) @2017-10-16 10:33:39.032168\n",
      "Tournament 1007 Primary(W) vs Sparring(B|policy_model.minidb) started @2017-10-16 10:36:36.889712\n",
      "Tournament 1007 Finished with Primary(W) 11:5 Sparring(B) @2017-10-16 10:38:57.177525\n",
      "Tournament 1008 Primary(W) vs Sparring(B|policy_model_RL_21.minidb) started @2017-10-16 10:42:00.916612\n",
      "Tournament 1008 Finished with Primary(W) 10:6 Sparring(B) @2017-10-16 10:44:20.643481\n"
     ]
    }
   ],
   "source": [
    "for tournament in range(PRIMARY_PRE_TRAINED_ITERS, PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS):\n",
    "    # Every 500 tournament, copy current player to opponent. i.e. checkpoint\n",
    "    if tournament > 0 and tournament % 20 == 0:\n",
    "        pe.save_to_db(\"minidb\", os.path.join(PRIMARY_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(PRIMARY_SAVE_FOLDER))\n",
    "        pe.save_to_db(\"minidb\", os.path.join(SPARR_LOAD_FOLDER, \"policy_model_RL_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+tournament)), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(SPARR_LOAD_FOLDER))\n",
    "    \n",
    "    # Randomly change color of player\n",
    "    PRIMARY_PLAYER = np.random.choice(['B','W'])\n",
    "    if PRIMARY_PLAYER == 'B':\n",
    "        SPARRING_PLAYER = 'W'\n",
    "    else:\n",
    "        SPARRING_PLAYER = 'B'\n",
    "    \n",
    "    # Randomly pickup sparring partner\n",
    "    workspace.SwitchWorkspace(SPARR_WORKSPACE, True)\n",
    "    sparring_param_file = np.random.choice(os.listdir(SPARR_LOAD_FOLDER))\n",
    "    sparring_predict_net = pe.prepare_prediction_net(os.path.join(SPARR_LOAD_FOLDER, sparring_param_file),\n",
    "                                                     \"minidb\", device_option=device_opts)\n",
    "    print('Tournament {} Primary({}) vs Sparring({}|{}) started @{}'\n",
    "          .format(tournament, PRIMARY_PLAYER, SPARRING_PLAYER, sparring_param_file, datetime.now()))\n",
    "\n",
    "    \n",
    "    # Initialize game board and game state\n",
    "    game_state = [ GameState() for i in range(GAMES_BATCHES) ]\n",
    "    game_result = [0] * GAMES_BATCHES # 0 - Not Ended; BLACK - Black Wins; WHITE - White Wins\n",
    "    p = Preprocess(DEFAULT_FEATURES) # Singleton\n",
    "    history = [ [] for i in range(GAMES_BATCHES) ] # history[n][step] stores tuple of (player, x, y, board[n])\n",
    "    board = None # The preprocessed board with shape Nx48x19x19\n",
    "    \n",
    "    # for each step in all games\n",
    "    for step in range(0,500):\n",
    "        \n",
    "        # Preprocess the board\n",
    "        board = np.concatenate([p.state_to_tensor(game_state[i]).astype(np.float32) for i in range(GAMES_BATCHES)])\n",
    "\n",
    "        if step % 2 == 0:\n",
    "            current_player = BLACK\n",
    "            current_color = 'B'\n",
    "        else:\n",
    "            current_player = WHITE\n",
    "            current_color = 'W'\n",
    "\n",
    "        if step % 2 == (PRIMARY_PLAYER == 'W'): # if step %2 == 0 and Primary is Black, or vice versa.\n",
    "            # primary player make move\n",
    "            workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "            workspace.FeedBlob('data', board, device_option=device_opts)\n",
    "            workspace.RunNet(primary_predict_net)\n",
    "        else:\n",
    "            # sparring partner make move\n",
    "            workspace.SwitchWorkspace(SPARR_WORKSPACE)\n",
    "            workspace.FeedBlob('data', board, device_option=device_opts)\n",
    "            workspace.RunNet(sparring_predict_net)\n",
    "\n",
    "        predict = workspace.FetchBlob('predict') # [0.01, 0.02, ...] in shape (N,361)\n",
    "\n",
    "        for i in range(GAMES_BATCHES):\n",
    "            if game_result[i]: # game end\n",
    "                continue\n",
    "            else: # game not end\n",
    "                legal_moves = [ x*19+y for (x,y) in game_state[i].get_legal_moves(include_eyes=False)] # [59, 72, ...] in 1D\n",
    "                if len(legal_moves) > 0: # at least 1 legal move\n",
    "                    probabilities = predict[i][legal_moves] # [0.02, 0.01, ...]\n",
    "                    # use numpy.random.choice to randomize the step,\n",
    "                    # otherwise use np.argmax to get best choice\n",
    "                    # current_choice = legal_moves[np.argmax(probabilities)]\n",
    "                    if np.sum(probabilities) > 0:\n",
    "                        current_choice = np.random.choice(legal_moves, 1, p=probabilities/np.sum(probabilities))[0]\n",
    "                    else:\n",
    "                        current_choice = np.random.choice(legal_moves, 1)[0]\n",
    "                    (x, y) = (current_choice/19, current_choice%19)\n",
    "                    history[i].append((current_color, current_choice, board[i]))\n",
    "                    game_state[i].do_move(action = (x, y), color = current_player) # End of Game?\n",
    "                    #print('game({}) step({}) {} move({},{})'.format(i, step, current_color, x, y))\n",
    "                else:\n",
    "                    game_state[i].do_move(action = PASS, color = current_player)\n",
    "                    #print('game({}) step({}) {} PASS'.format(i, step, current_color))\n",
    "                    game_result[i] = game_state[i].is_end_of_game\n",
    "\n",
    "        if np.all(game_result):\n",
    "            break\n",
    "    \n",
    "    # Get the winner\n",
    "    winner = [ GetWinner(game_state[i]) for i in range(GAMES_BATCHES) ] # B+, W+, T\n",
    "    print('Tournament {} Finished with Primary({}) {}:{} Sparring({}) @{}'.\n",
    "          format(tournament, PRIMARY_PLAYER, sum(np.char.count(winner, PRIMARY_PLAYER)),\n",
    "                 sum(np.char.count(winner, SPARRING_PLAYER)), SPARRING_PLAYER, datetime.now()))\n",
    "    \n",
    "    # Save the games(optional)\n",
    "    for i in range(GAMES_BATCHES):\n",
    "        filename = os.path.join(\n",
    "            os.path.expanduser('~'), 'python', 'tutorial_files','selfplay',\n",
    "            '({}_{}_{})vs({})_{}_{}_{}'.format(PRIMARY_CONV_LEVEL, PRIMARY_FILTERS, PRIMARY_PRE_TRAINED_ITERS+tournament,\n",
    "                                            sparring_param_file, i, winner[i],\n",
    "                                            datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")))\n",
    "        #print(filename)\n",
    "        WriteBackSGF(winner, history[i], filename)\n",
    "    \n",
    "    # After each tournament, learn from the winner\n",
    "    if BASE_LR != 0:\n",
    "        LearnFromWinningGames(history, winner, mini_batch=TRAIN_BATCHES, device_opts=device_opts)\n",
    "    \n",
    "    # And learn from negative examples\n",
    "    if NEGATIVE_BASE_LR != 0:\n",
    "        LearnFromLosingGames(history, winner, mini_batch=TRAIN_BATCHES, device_opts=device_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TOURNAMENT_ITERS>0 :\n",
    "    pe.save_to_db(\"minidb\", os.path.join(PRIMARY_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "    print('Results saved to {}'.format(PRIMARY_SAVE_FOLDER))\n",
    "    pe.save_to_db(\"minidb\", os.path.join(SPARR_LOAD_FOLDER, \"policy_model_RL_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS)), pe_meta)\n",
    "    print('Results saved to {}'.format(SPARR_LOAD_FOLDER))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "315px",
    "width": "367px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "544px",
    "left": "0px",
    "right": "1723px",
    "top": "107px",
    "width": "130px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
