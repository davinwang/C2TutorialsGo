{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock AlphaGo (2) - Policy Network\n",
    "In this notebook, we will build the model of AlphaGo's Policy Network, which is a dCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named caffe2_pybind11_state_gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from 60000 to 100000 iterations\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from caffe2.python import core, model_helper, workspace, brew, utils\n",
    "from caffe2.proto import caffe2_pb2\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "CONV_LEVEL = 4 # [3,13(alphago)] How many CNN will be used in the model\n",
    "FILTERS = 128 # 128/192(alphago)/256/384 How many K will be used in the model\n",
    "BASE_LR = -0.003 # (-0.01,0) The base Learning Rate, alphago uses -0.003 and half the number every 80m steps\n",
    "\n",
    "DEV_OPT = core.DeviceOption(caffe2_pb2.CPU, 0) # no warranty for GPU option\n",
    "TRAIN_BATCHES = 32 # how many samples will be trained within one mini-batch, depends on your hardware\n",
    "PRE_TRAINED_ITERS = 60000 # [0, infinity) how many batches the model has been trained before\n",
    "SKIP_TRAINED_DATA = 0 # [0, infinity) if this is a resumed training, how many input data will be skipped\n",
    "TRAIN_ITERS = 40000 # [0, infinity) how many batches the model will be trained\n",
    "TEST_BATCHES = 100 # how many samples will be tested within one mini-batch\n",
    "TEST_ITERS = 1000 # how many batches the model will be tested\n",
    "\n",
    "ROOT_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','go','param') # folder stores the loss/accuracy log\n",
    "DATA_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','go')\n",
    "TRAIN_DATA = os.path.join(DATA_FOLDER,'kgs_2015') # db folder stores the preprocessed games\n",
    "TEST_DATA = os.path.join(DATA_FOLDER,'test_data') # db folder stores the preprocessed games\n",
    "\n",
    "# if this is a resumed training, where to load the init_param from\n",
    "LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"conv={}-k={}-iter={}\".format(CONV_LEVEL,FILTERS,PRE_TRAINED_ITERS))\n",
    "LOAD_INIT_NET = os.path.join(LOAD_FOLDER, \"param_init_net.pb\")\n",
    "LOAD_PREDICT_NET = os.path.join(LOAD_FOLDER, \"predict_net.pb\")\n",
    "\n",
    "# if the model will be saved for future resume training, where to store it\n",
    "SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"conv={}-k={}-iter={}\".format(CONV_LEVEL,FILTERS,PRE_TRAINED_ITERS+TRAIN_ITERS))\n",
    "SAVE_INIT_NET = os.path.join(SAVE_FOLDER, \"param_init_net.pb\")\n",
    "SAVE_PREDICT_NET = os.path.join(SAVE_FOLDER, \"predict_net.pb\")\n",
    "\n",
    "workspace.ResetWorkspace(ROOT_FOLDER)\n",
    "\n",
    "print('Training model from {} to {} iterations'.format(PRE_TRAINED_ITERS,PRE_TRAINED_ITERS+TRAIN_ITERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Neural Network Architecture\n",
    "Refer to AlphaGo\n",
    "> The input to the policy network is a 19 x 19 x 48 image stack consisting of 48 feature planes. The first hidden layer zero-pads the input into a 23 x 23 image, then convolves k filters of kernel size 5 x 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 x 21 image, then convolves k filters of kernel size 3x3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 x 1 with stride 1, with a different bias for each position, and applies a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input\n",
    "This program requires input data in shape of 48 x 19 x 19, which is preprocessed from SGF files, and label of scalar, which represents the next move.\n",
    "    The board of Go is symmetric in 8 directions, so this method can be enhanced to transpose and mirror the input data in 8 directions. According to DeepMind, training the model with symmetric data in 8 directions will increase the accuracy data by around 1-2% which is significant. However, it also takes 8 times longer to train the model. Spending same amount of time in Reinforced Training instead of symmetric data may achieve better winning rate. AlphaGo didn't use the symmetric data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddInput(model, batch_size, db, db_type):\n",
    "    # Data is stored in INT8 while label is stored in UINT16\n",
    "    # This will save disk storage\n",
    "    data_int8, label_uint16 = model.TensorProtosDBInput(\n",
    "        [], ['data_int8', 'label_uint16'], batch_size=batch_size,\n",
    "        db=db, db_type=db_type)\n",
    "    # cast to float\n",
    "    data = model.Cast(data_int8, 'data', to=core.DataType.FLOAT)\n",
    "    # cast to int\n",
    "    label = model.Cast(label_uint16, 'label', to=core.DataType.INT32)\n",
    "    # don't need the gradient for the backward pass\n",
    "    data = model.StopGradient(data, data)\n",
    "    label = model.StopGradient(label, label)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddConvModel(model, data):\n",
    "    # Layer 1: 48 x 19 x 19 -pad-> 48 x 23 x 23 -conv-> 192 x 19 x 19\n",
    "    pad1 = model.PadImage(data, 'pad1', pad_t=2, pad_l=2, pad_b=2, pad_r=2, mode=\"constant\", value=0.)\n",
    "    conv1 = brew.conv(model, pad1, 'conv1', dim_in=48, dim_out=FILTERS, kernel=5)\n",
    "    relu1 = brew.relu(model, conv1, 'relu1')\n",
    "    # Layer 2-12: 192 x 19 x 19 -pad-> 192 x 21 x 21 -conv-> 192 x 19 x 19\n",
    "    if CONV_LEVEL > 2:\n",
    "        pad2 = model.PadImage(relu1, 'pad2', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv2 = brew.conv(model, pad2, 'conv2', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu2 = brew.relu(model, conv2, 'relu2')\n",
    "        relu = relu2\n",
    "    #\n",
    "    if CONV_LEVEL > 3:\n",
    "        pad3 = model.PadImage(relu2, 'pad3', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv3 = brew.conv(model, pad3, 'conv3', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu3 = brew.relu(model, conv3, 'relu3')\n",
    "        relu = relu3\n",
    "    #\n",
    "    if CONV_LEVEL > 4:\n",
    "        pad4 = model.PadImage(relu3, 'pad4', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv4 = brew.conv(model, pad4, 'conv4', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu4 = brew.relu(model, conv4, 'relu4')\n",
    "        relu = relu4\n",
    "    #\n",
    "    if CONV_LEVEL > 5:\n",
    "        pad5 = model.PadImage(relu4, 'pad5', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv5 = brew.conv(model, pad5, 'conv5', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu5 = brew.relu(model, conv5, 'relu5')\n",
    "        relu = relu5\n",
    "    #\n",
    "    if CONV_LEVEL > 6:\n",
    "        pad6 = model.PadImage(relu5, 'pad6', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv6 = brew.conv(model, pad6, 'conv6', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu6 = brew.relu(model, conv6, 'relu6')\n",
    "        relu = relu6\n",
    "    #\n",
    "    if CONV_LEVEL > 7:\n",
    "        pad7 = model.PadImage(relu6, 'pad7', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv7 = brew.conv(model, pad7, 'conv7', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu7 = brew.relu(model, conv7, 'relu7')\n",
    "        relu = relu7\n",
    "    #\n",
    "    if CONV_LEVEL > 8:\n",
    "        pad8 = model.PadImage(relu7, 'pad8', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv8 = brew.conv(model, pad8, 'conv8', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu8 = brew.relu(model, conv8, 'relu8')\n",
    "        relu = relu8\n",
    "    #\n",
    "    if CONV_LEVEL > 9:\n",
    "        pad9 = model.PadImage(relu8, 'pad9', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv9 = brew.conv(model, pad9, 'conv9', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu9 = brew.relu(model, conv9, 'relu9')\n",
    "        relu = relu9\n",
    "    #\n",
    "    if CONV_LEVEL > 10:\n",
    "        pad10 = model.PadImage(relu9, 'pad10', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv10 = brew.conv(model, pad10, 'conv10', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu10 = brew.relu(model, conv10, 'relu10')\n",
    "        relu = relu10\n",
    "    #\n",
    "    if CONV_LEVEL > 11:\n",
    "        pad11 = model.PadImage(relu10, 'pad11', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv11 = brew.conv(model, pad11, 'conv11', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu11 = brew.relu(model, conv11, 'relu11')\n",
    "        relu = relu11\n",
    "    #\n",
    "    if CONV_LEVEL > 12:\n",
    "        pad12 = model.PadImage(relu11, 'pad12', pad_t=1, pad_l=1, pad_b=1, pad_r=1, mode=\"constant\", value=0.)\n",
    "        conv12 = brew.conv(model, pad12, 'conv12', dim_in=FILTERS, dim_out=FILTERS, kernel=3)\n",
    "        relu12 = brew.relu(model, conv12, 'relu12')\n",
    "        relu = relu12\n",
    "    # Layer 13: 192 x 19 x 19 -conv-> 1 x 19 x 19 -softmax-> 361\n",
    "    conv13 = brew.conv(model, relu, 'conv13', dim_in=FILTERS, dim_out=1, kernel=1)\n",
    "    ## todo: bias layer?\n",
    "    predict = brew.softmax(model, conv13, 'predict')\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Please note predict is 4 dimensional tensor in shape of N x 1 x 19 x 19, and label is 2 dimensional tensor in shape of N x 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddAccuracy(model, predict, label):\n",
    "    \"\"\"Adds an accuracy op to the model\"\"\"\n",
    "    predict_2d, _ = model.Reshape(['predict'], ['predict_2d', '_'], shape=(0,-1))\n",
    "    accuracy = brew.accuracy(model, [predict_2d, label], \"accuracy\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Operator (Backward Propagation)\n",
    "The training operator is almost same as MNIST. Refer to AlphaGo\n",
    ">The step-size \u000b",
    " was initialized to 0.003 and was halved every 80\n",
    "million training steps, without momentum terms, and a mini-batch size of m = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddTrainingOperators(model, softmax, label):\n",
    "    \"\"\"Adds training operators to the model.\"\"\"\n",
    "    xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "    # compute the expected loss\n",
    "    loss = model.AveragedLoss(xent, \"loss\")\n",
    "    # track the accuracy of the model\n",
    "    AddAccuracy(model, softmax, label)\n",
    "    # use the average loss we just computed to add gradient operators to the model\n",
    "    model.AddGradientOperators([loss])\n",
    "    # do a simple stochastic gradient descent\n",
    "    ITER = brew.iter(model, \"iter\")\n",
    "    # set the learning rate schedule\n",
    "    LR = model.LearningRate(\n",
    "        ITER, \"LR\", base_lr=BASE_LR, policy=\"fixed\", stepsize=1, gamma=0.999 ) # when policy=fixed, stepsize and gamma are ignored\n",
    "    # ONE is a constant value that is used in the gradient update. We only need\n",
    "    # to create it once, so it is explicitly placed in param_init_net.\n",
    "    ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "    # Now, for each parameter, we do the gradient updates.\n",
    "    for param in model.params:\n",
    "        # Note how we get the gradient of each parameter - ModelHelper keeps\n",
    "        # track of that.\n",
    "        param_grad = model.param_to_grad[param]\n",
    "        # The update is a simple weighted sum: param = param + param_grad * LR\n",
    "        model.WeightedSum([param, ONE, param_grad, LR], param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace the training progress\n",
    "Print 'accuracy' and 'loss' to file and we can monitor them elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddBookkeepingOperators(model):\n",
    "    \"\"\"This adds a few bookkeeping operators that we can inspect later.\n",
    "    These operators do not affect the training procedure: they only collect\n",
    "    statistics and prints them to file or to logs.\n",
    "    \"\"\"    \n",
    "    # Print basically prints out the content of the blob. to_file=1 routes the\n",
    "    # printed output to a file. The file is going to be stored under\n",
    "    #     root_folder/[blob name]\n",
    "    model.Print('accuracy', [], to_file=1)\n",
    "    model.Print('loss', [], to_file=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the actual network\n",
    "### Train Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Skip model only has DBInput to waste the input\n",
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "skip_model = model_helper.ModelHelper(name=\"skip_model\", arg_scope=arg_scope, init_params=True)\n",
    "_d, _l = AddInput(\n",
    "    skip_model, batch_size=TRAIN_BATCHES,\n",
    "    db=TRAIN_DATA,\n",
    "    db_type='leveldb')\n",
    "# Initialize params and create network\n",
    "workspace.RunNetOnce(skip_model.param_init_net)\n",
    "workspace.CreateNet(skip_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Reshape.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Net: DBInput ==> Predict Net ==> Loss ==> Backward Propergation\n",
    "train_model = model_helper.ModelHelper(name=\"policy_train\", arg_scope=arg_scope, init_params=True)\n",
    "data, label = AddInput(\n",
    "    train_model, batch_size=TRAIN_BATCHES,\n",
    "    db=TRAIN_DATA,\n",
    "    db_type='leveldb')\n",
    "predict = AddConvModel(train_model, data)\n",
    "AddTrainingOperators(train_model, predict, label)\n",
    "AddBookkeepingOperators(train_model)\n",
    "# Initialize params and create network\n",
    "workspace.RunNetOnce(train_model.param_init_net)\n",
    "workspace.CreateNet(train_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Reshape.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Net: DBInput ==> Predict Net ==> Accuracy\n",
    "test_model = model_helper.ModelHelper(name=\"policy_test\", arg_scope=arg_scope, init_params=False)\n",
    "data, label = AddInput(\n",
    "    test_model, batch_size=TEST_BATCHES,\n",
    "    db=TEST_DATA,\n",
    "    db_type='leveldb')\n",
    "predict = AddConvModel(test_model, data)\n",
    "AddAccuracy(test_model, predict, label)\n",
    "# Initialize params and create network\n",
    "workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.CreateNet(test_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Net: Blob('data') ==> Predict Net ==> Blob('predict')\n",
    "deploy_model = model_helper.ModelHelper(name=\"policy_deploy\", arg_scope=arg_scope, init_params=False)\n",
    "AddConvModel(deploy_model, \"data\")\n",
    "# Initialize params and create network\n",
    "workspace.RunNetOnce(deploy_model.param_init_net)\n",
    "workspace.CreateNet(deploy_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training and testing\n",
    "### resume from last training\n",
    "    Training a multi-level CNN takes quite a long time. To pause-and-resume the training, set the PRE_TRAINED_ITERS so the program will start from where last time it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded from /home/wangd/python/tutorial_data/go/param/conv=4-k=128-iter=60000\n"
     ]
    }
   ],
   "source": [
    "def load_net(INIT_NET, PREDICT_NET, device_opts=DEV_OPT):\n",
    "    init_def = caffe2_pb2.NetDef()\n",
    "    with open(INIT_NET, 'r') as f:\n",
    "        init_def.ParseFromString(f.read())\n",
    "        init_def.device_option.CopyFrom(device_opts)\n",
    "        workspace.RunNetOnce(init_def.SerializeToString())\n",
    "    net_def = caffe2_pb2.NetDef()\n",
    "    with open(PREDICT_NET, 'r') as f:\n",
    "        net_def.ParseFromString(f.read())\n",
    "        net_def.device_option.CopyFrom(device_opts)\n",
    "        workspace.CreateNet(net_def.SerializeToString(), overwrite=True)\n",
    "        \n",
    "def save_net(INIT_NET, PREDICT_NET, model) :\n",
    "    with open(PREDICT_NET, 'wb') as f:\n",
    "        f.write(model.net._net.SerializeToString())\n",
    "    init_net = caffe2_pb2.NetDef()\n",
    "    for param in model.params:\n",
    "        blob = workspace.FetchBlob(param)\n",
    "        shape = blob.shape\n",
    "        op = core.CreateOperator(\"GivenTensorFill\", [], [param],arg=[ utils.MakeArgument(\"shape\", shape),utils.MakeArgument(\"values\", blob)])\n",
    "        init_net.op.extend([op])\n",
    "    init_net.op.extend([core.CreateOperator(\"ConstantFill\", [], [\"data\"], shape=(1,30,30))])\n",
    "    with open(INIT_NET, 'wb') as f:\n",
    "        f.write(init_net.SerializeToString())\n",
    "        \n",
    "if PRE_TRAINED_ITERS > 0:\n",
    "    load_net(LOAD_INIT_NET, LOAD_PREDICT_NET)\n",
    "    print('Params loaded from {}'.format(LOAD_FOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture output # Jupyter magic command to capture the output\n",
    "\n",
    "if TRAIN_ITERS > 0:\n",
    "    # skip the data which should not be trained again\n",
    "    for i in range(SKIP_TRAINED_DATA):\n",
    "        workspace.RunNet(skip_model.net)\n",
    "    \n",
    "    # set the number of iterations and track the accuracy & loss\n",
    "    accuracy = np.zeros(TRAIN_ITERS)\n",
    "    loss = np.zeros(TRAIN_ITERS)\n",
    "    # Now, run the network \n",
    "    for i in range(TRAIN_ITERS):\n",
    "        workspace.RunNet(train_model.net)\n",
    "        accuracy[i] = workspace.FetchBlob('accuracy')\n",
    "        loss[i] = workspace.FetchBlob('loss')\n",
    "        # checkpoint every 1000 iterations\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            if not os.path.exists(SAVE_FOLDER):\n",
    "                os.makedirs(SAVE_FOLDER)\n",
    "            save_net(SAVE_INIT_NET, SAVE_PREDICT_NET, deploy_model)\n",
    "            print('Checkpoint {} saved to {}'.format(i,SAVE_FOLDER))\n",
    "    # After the execution is done, plot the values.\n",
    "    pyplot.plot(loss, 'b')\n",
    "    pyplot.plot(accuracy, 'r')\n",
    "    pyplot.legend(('Loss', 'Accuracy'), loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = np.zeros(TEST_ITERS)\n",
    "for i in range(TEST_ITERS):\n",
    "    workspace.RunNet(test_model.net)\n",
    "    test_accuracy[i] = workspace.FetchBlob('accuracy')\n",
    "# After the execution is done, let's plot the values.\n",
    "pyplot.plot(test_accuracy, 'r')\n",
    "pyplot.title('Acuracy over test batches.')\n",
    "print('test_accuracy: %f' % test_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the work for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TRAIN_ITERS > 0:\n",
    "    if not os.path.exists(SAVE_FOLDER):\n",
    "        os.makedirs(SAVE_FOLDER)\n",
    "    save_net(SAVE_INIT_NET, SAVE_PREDICT_NET, deploy_model)\n",
    "    print('Params saved to {}'.format(SAVE_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(test_accuracy, 'r')\n",
    "pyplot.title('Acuracy over test batches.')\n",
    "print('test_accuracy: %f' % test_accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "856px",
    "left": "0px",
    "right": "20px",
    "top": "107px",
    "width": "179px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
