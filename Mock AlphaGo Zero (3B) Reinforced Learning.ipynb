{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock AlphaGo Zero (3B) Reinforced Learning\n",
    "In this notebook, we will train the policy network by letting them compete each other according to DeepMind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n",
      "WARNING:root:Debug message: No module named caffe2_pybind11_state_gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in CPU mode\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from caffe2.python import core, model_helper, workspace, brew, utils\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from sgfutil import BOARD_POSITION\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# how many games will be run in one minibatch\n",
    "GAMES_BATCHES = 16 # [1,infinity) depends on your hardware\n",
    "SEARCH_WIDE = 1600 # [1, infinity) for each step, run MCTS to obtain better distribution\n",
    "# how many iterations for this tournament\n",
    "TOURNAMENT_ITERS = 1 # [1,infinity)\n",
    "\n",
    "if workspace.has_gpu_support:\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CUDA, workspace.GetDefaultGPUID())\n",
    "    print('Running in GPU mode on default device {}'.format(workspace.GetDefaultGPUID()))\n",
    "else :\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CPU, 0)\n",
    "    print('Running in CPU mode')\n",
    "\n",
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "\n",
    "ROOT_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','zero','param') # folder stores the loss/accuracy log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only 3 features are needed for AlphaGo Zero\n",
    "# 0 - Player Stone, 1 - Opponent Stone, 3 - Current Player Color\n",
    "DEFAULT_FEATURES = [\"board\", \"color\"]\n",
    "\n",
    "# reverse the index of player/opponent\n",
    "# 0,2,4,6... are player, 1,3,5,7... are opponent\n",
    "OPPONENT_INDEX = [1,0,3,2,5,4,7,6,9,8,11,10,13,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to differentiate primary player and sparring partner. Primary player will learn from the game result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from 0 to 1 iterations\n"
     ]
    }
   ],
   "source": [
    "### Config for primary player\n",
    "PRIMARY_WORKSPACE = os.path.join(ROOT_FOLDER, 'primary')\n",
    "PRIMARY_RES_BLOCKS = 1 # [1,19(AlphaGo Zero),39]\n",
    "PRIMARY_FILTERS = 128 # [128, 192, 256(AlphaGo Zero), 384]\n",
    "PRIMARY_PRE_TRAINED_ITERS = 0\n",
    "# before traning, where to load the params\n",
    "PRIMARY_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-res={}-k={}-iter={}\"\n",
    "                                   .format(PRIMARY_RES_BLOCKS,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS))\n",
    "BASE_LR = -0.01 # (-0.01,0) The base Learning Rate; 0 to disable it.\n",
    "TRAIN_BATCHES = 16 # how many samples will be trained within one mini-batch, depends on your hardware\n",
    "# after training, where to store the params\n",
    "PRIMARY_SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"RL-res={}-k={}-iter={}\"\n",
    "                           .format(PRIMARY_RES_BLOCKS,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))\n",
    "if not os.path.exists(PRIMARY_SAVE_FOLDER):\n",
    "    os.makedirs(PRIMARY_SAVE_FOLDER)\n",
    "\n",
    "### Config for sparring partner\n",
    "SPARR_WORKSPACE = os.path.join(ROOT_FOLDER, 'sparring')\n",
    "SPARR_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"res={}-k={}-iter={}\".format(1,128,1))\n",
    "\n",
    "print('Training model from {} to {} iterations'.format(PRIMARY_PRE_TRAINED_ITERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modelingZero import AddResNetModel, AddSoftmax, AddTrainingOperators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the actual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe2.python.predictor.predictor_exporter as pe\n",
    "\n",
    "data = np.empty(shape=(TRAIN_BATCHES,17,19,19), dtype=np.float32)\n",
    "expect = np.empty(shape=(TRAIN_BATCHES,362), dtype=np.float32) # expected distribution of probability\n",
    "reward = np.empty(shape=(TRAIN_BATCHES,), dtype=np.float32) # scalar values between [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary player\n",
    ">Train Net: Blob('data','label') ==> Predict Net ==> Loss ==> Backward Propergation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n"
     ]
    }
   ],
   "source": [
    "workspace.SwitchWorkspace(PRIMARY_WORKSPACE, True)\n",
    "\n",
    "with core.DeviceScope(device_opts):\n",
    "    workspace.FeedBlob(\"data\", data)\n",
    "    workspace.FeedBlob('expect', expect)\n",
    "    workspace.FeedBlob('reward', reward)\n",
    "    # for learning from winner\n",
    "    primary_train_model = model_helper.ModelHelper(name=\"primary_train_model\", arg_scope=arg_scope, init_params=True)\n",
    "    predict, value = AddResNetModel(primary_train_model, 'data', num_blocks=PRIMARY_RES_BLOCKS, filters=PRIMARY_FILTERS)\n",
    "    AddTrainingOperators(primary_train_model, predict, None, 'expect', value, 'reward', base_lr=BASE_LR)\n",
    "    workspace.RunNetOnce(primary_train_model.param_init_net)\n",
    "    workspace.CreateNet(primary_train_model.net, overwrite=True)\n",
    "    # \n",
    "    primary_predict_net = pe.prepare_prediction_net(os.path.join(PRIMARY_LOAD_FOLDER, \"policy_model.minidb\"), \"minidb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `LearnFromWinner` takes the result of tournament and train primary player with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LearnFromWinningGames(history, winner, mini_batch=TRAIN_BATCHES):\n",
    "    data = np.empty(shape=(mini_batch,17,19,19), dtype=np.float32)\n",
    "    label = np.empty(shape=(mini_batch,), dtype=np.int32)\n",
    "    #iter = 0\n",
    "    k = 0\n",
    "    for i in range(len(winner)):\n",
    "        #print('Learning {} steps in {} of {} games'.format(iter * TRAIN_BATCHES, i, GAMES_BATCHES))\n",
    "        for step in history[i]:\n",
    "            if (step[0] == 'B' and winner[i] == 'B+') or (step[0] == 'W' and winner[i] == 'W+'):\n",
    "                data[k] = step[2]\n",
    "                label[k] = step[1]\n",
    "                k += 1\n",
    "                #iter += 1\n",
    "                if k == mini_batch:\n",
    "                    k = 0\n",
    "                    workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "                    with core.DeviceScope(device_opts):\n",
    "                        workspace.FeedBlob(\"data\", data)\n",
    "                        workspace.FeedBlob(\"label\", label)\n",
    "                        workspace.RunNet(primary_train_model.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparring partner\n",
    "    Load on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tournament and training\n",
    ">We use a reward function $r(s)$ that is zero for all non-terminal time-steps $t < T$.\n",
    "The outcome $z_t = \\pm r(s_T)$ is the terminal reward at the end of the game from the perspective of the\n",
    "current player at time-step $t$: $+1$ for winning and $-1$ for losing. Weights are then updated at each\n",
    "time-step $t$ by stochastic gradient ascent in the direction that maximizes expected outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from go import GameState, BLACK, WHITE, EMPTY, PASS\n",
    "from preprocessing import Preprocess\n",
    "from datetime import datetime\n",
    "from sgfutil import GetWinner, WriteBackSGF\n",
    "import sgf\n",
    "\n",
    "np.random.seed(datetime.now().microsecond)\n",
    "\n",
    "# construct the model to be exported\n",
    "pe_meta = pe.PredictorExportMeta(\n",
    "    predict_net=primary_predict_net.Proto(),\n",
    "    parameters=[str(b) for b in primary_train_model.params],\n",
    "    inputs=[\"data\"],\n",
    "    outputs=[\"softmax\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tournament 0 Primary(B) vs Sparring(W|policy_model.minidb) started @2017-11-01 18:30:04.795610\n",
      "Traceback for operator 1 in network policy_deploy_1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at conv_op_impl.h:46] C == filter.dim32(1) * group_. Convolution op: input channels does not match: # of input channels 4 is not equal to kernel channels * group:17*1 Error from operator: \ninput: \"pad1\" input: \"conv1_w\" input: \"conv1_b\" output: \"conv1\" name: \"\" type: \"Conv\" arg { name: \"kernel\" i: 3 } arg { name: \"exhaustive_search\" i: 0 } arg { name: \"order\" s: \"NCHW\" } device_option { device_type: 0 cuda_gpu_id: 0 } engine: \"CUDNN\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-221bd966c784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeedBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_predict_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# sparring partner make move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/caffe2/python/workspace.py\u001b[0m in \u001b[0;36mRunNet\u001b[0;34m(name, num_iter, allow_fail)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWorkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_failed_op_net_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mGetNetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mStringifyNetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fail\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/caffe2/python/workspace.py\u001b[0m in \u001b[0;36mCallWithExceptionIntercept\u001b[0;34m(func, op_id_fetcher, net_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCallWithExceptionIntercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_id_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mop_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_id_fetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at conv_op_impl.h:46] C == filter.dim32(1) * group_. Convolution op: input channels does not match: # of input channels 4 is not equal to kernel channels * group:17*1 Error from operator: \ninput: \"pad1\" input: \"conv1_w\" input: \"conv1_b\" output: \"conv1\" name: \"\" type: \"Conv\" arg { name: \"kernel\" i: 3 } arg { name: \"exhaustive_search\" i: 0 } arg { name: \"order\" s: \"NCHW\" } device_option { device_type: 0 cuda_gpu_id: 0 } engine: \"CUDNN\""
     ]
    }
   ],
   "source": [
    "for tournament in range(PRIMARY_PRE_TRAINED_ITERS, PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS):\n",
    "    # Every 500 tournament, copy current player to opponent. i.e. checkpoint\n",
    "    if tournament > 0 and tournament % 20 == 0:\n",
    "        pe.save_to_db(\"minidb\", os.path.join(PRIMARY_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(PRIMARY_SAVE_FOLDER))\n",
    "        pe.save_to_db(\"minidb\", os.path.join(SPARR_LOAD_FOLDER, \"policy_model_RL_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+tournament)), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(SPARR_LOAD_FOLDER))\n",
    "    \n",
    "    # Randomly change color of player\n",
    "    PRIMARY_PLAYER = np.random.choice(['B','W'])\n",
    "    if PRIMARY_PLAYER == 'B':\n",
    "        SPARRING_PLAYER = 'W'\n",
    "    else:\n",
    "        SPARRING_PLAYER = 'B'\n",
    "    \n",
    "    # Randomly pickup sparring partner\n",
    "    workspace.SwitchWorkspace(SPARR_WORKSPACE, True)\n",
    "    sparring_param_file = np.random.choice(os.listdir(SPARR_LOAD_FOLDER))\n",
    "    with core.DeviceScope(device_opts):\n",
    "        sparring_predict_net = pe.prepare_prediction_net(os.path.join(SPARR_LOAD_FOLDER, sparring_param_file), \"minidb\")\n",
    "    print('Tournament {} Primary({}) vs Sparring({}|{}) started @{}'\n",
    "          .format(tournament, PRIMARY_PLAYER, SPARRING_PLAYER, sparring_param_file, datetime.now()))\n",
    "\n",
    "    \n",
    "    # Initialize game board and game state\n",
    "    game_state = [ GameState() for i in range(GAMES_BATCHES) ]\n",
    "    game_result = [0] * GAMES_BATCHES # 0 - Not Ended; BLACK - Black Wins; WHITE - White Wins\n",
    "    p = Preprocess(DEFAULT_FEATURES) # Singleton\n",
    "    history = [ [] for i in range(GAMES_BATCHES) ] # history[n][step] stores tuple of (player, x, y, board[n])\n",
    "    board = None # The preprocessed board with shape Nx17x19x19\n",
    "    \n",
    "    # for each step in all games\n",
    "    for step in range(0,722):\n",
    "        \n",
    "        # Preprocess the board\n",
    "        board = np.concatenate([p.state_to_tensor(game_state[i]).astype(np.float32) for i in range(GAMES_BATCHES)])\n",
    "\n",
    "        if step % 2 == 0:\n",
    "            current_player = BLACK\n",
    "            current_color = 'B'\n",
    "        else:\n",
    "            current_player = WHITE\n",
    "            current_color = 'W'\n",
    "\n",
    "        if step % 2 == (PRIMARY_PLAYER == 'W'): # if step %2 == 0 and Primary is Black, or vice versa.\n",
    "            # primary player make move\n",
    "            workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "            with core.DeviceScope(device_opts):\n",
    "                workspace.FeedBlob('data', board)\n",
    "                workspace.RunNet(primary_predict_net)\n",
    "        else:\n",
    "            # sparring partner make move\n",
    "            workspace.SwitchWorkspace(SPARR_WORKSPACE)\n",
    "            with core.DeviceScope(device_opts):\n",
    "                workspace.FeedBlob('data', board)\n",
    "                workspace.RunNet(sparring_predict_net)\n",
    "\n",
    "        predict = workspace.FetchBlob('softmax') # [0.01, 0.02, ...] in shape (N,361)\n",
    "\n",
    "        for i in range(GAMES_BATCHES):\n",
    "            if game_result[i]: # game end\n",
    "                continue\n",
    "            else: # game not end\n",
    "                legal_moves = [ x*19+y for (x,y) in game_state[i].get_legal_moves(include_eyes=False)] # [59, 72, ...] in 1D\n",
    "                if len(legal_moves) > 0: # at least 1 legal move\n",
    "                    probabilities = predict[i][legal_moves] # [0.02, 0.01, ...]\n",
    "                    # use numpy.random.choice to randomize the step,\n",
    "                    # otherwise use np.argmax to get best choice\n",
    "                    # current_choice = legal_moves[np.argmax(probabilities)]\n",
    "                    if np.sum(probabilities) > 0:\n",
    "                        current_choice = np.random.choice(legal_moves, 1, p=probabilities/np.sum(probabilities))[0]\n",
    "                    else:\n",
    "                        current_choice = np.random.choice(legal_moves, 1)[0]\n",
    "                    (x, y) = (current_choice/19, current_choice%19)\n",
    "                    history[i].append((current_color, current_choice, board[i]))\n",
    "                    game_state[i].do_move(action = (x, y), color = current_player) # End of Game?\n",
    "                    #print('game({}) step({}) {} move({},{})'.format(i, step, current_color, x, y))\n",
    "                else:\n",
    "                    game_state[i].do_move(action = PASS, color = current_player)\n",
    "                    #print('game({}) step({}) {} PASS'.format(i, step, current_color))\n",
    "                    game_result[i] = game_state[i].is_end_of_game\n",
    "\n",
    "        if np.all(game_result):\n",
    "            break\n",
    "    \n",
    "    # Get the winner\n",
    "    winner = [ GetWinner(game_state[i]) for i in range(GAMES_BATCHES) ] # B+, W+, T\n",
    "    print('Tournament {} Finished with Primary({}) {}:{} Sparring({}) @{}'.\n",
    "          format(tournament, PRIMARY_PLAYER, sum(np.char.count(winner, PRIMARY_PLAYER)),\n",
    "                 sum(np.char.count(winner, SPARRING_PLAYER)), SPARRING_PLAYER, datetime.now()))\n",
    "    \n",
    "    # Save the games(optional)\n",
    "    for i in range(GAMES_BATCHES):\n",
    "        filename = os.path.join(\n",
    "            os.path.expanduser('~'), 'python', 'tutorial_files','selfplay',\n",
    "            '({}_{}_{})vs({})_{}_{}_{}'.format(PRIMARY_CONV_LEVEL, PRIMARY_FILTERS, PRIMARY_PRE_TRAINED_ITERS+tournament,\n",
    "                                            sparring_param_file, i, winner[i],\n",
    "                                            datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")))\n",
    "        WriteBackSGF(winner, history[i], filename)\n",
    "    \n",
    "    # After each tournament, learn from the winner\n",
    "    if BASE_LR != 0:\n",
    "        LearnFromWinningGames(history, winner, mini_batch=TRAIN_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TOURNAMENT_ITERS>0 :\n",
    "    pe.save_to_db(\"minidb\", os.path.join(PRIMARY_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "    print('Results saved to {}'.format(PRIMARY_SAVE_FOLDER))\n",
    "    pe.save_to_db(\"minidb\", os.path.join(SPARR_LOAD_FOLDER, \"policy_model_RL_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS)), pe_meta)\n",
    "    print('Results saved to {}'.format(SPARR_LOAD_FOLDER))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "315px",
    "width": "367px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "544px",
    "left": "0px",
    "right": "1723px",
    "top": "107px",
    "width": "130px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
