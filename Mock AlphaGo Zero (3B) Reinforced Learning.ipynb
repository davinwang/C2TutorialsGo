{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock AlphaGo Zero (3B) Reinforced Learning\n",
    "In this notebook, we will train the policy network by letting them compete each other according to DeepMind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in CPU mode\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from caffe2.python import core, model_helper, workspace, brew, utils\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from sgfutil import BOARD_POSITION\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# how many games will be run in one minibatch\n",
    "GAMES_BATCHES = 16 # [1,infinity) depends on your hardware\n",
    "SEARCH_WIDE = 1600 # [1, infinity) for each step, run MCTS to obtain better distribution\n",
    "# how many iterations for this tournament\n",
    "TOURNAMENT_ITERS = 10000 # [1,infinity)\n",
    "\n",
    "if workspace.has_gpu_support:\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CUDA, workspace.GetDefaultGPUID())\n",
    "    print('Running in GPU mode on default device {}'.format(workspace.GetDefaultGPUID()))\n",
    "else :\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CPU, 0)\n",
    "    print('Running in CPU mode')\n",
    "\n",
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "\n",
    "ROOT_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','zero','param') # folder stores the loss/accuracy log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to differentiate primary player and sparring partner. Primary player will learn from the game result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from 0 to 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "### Config for primary player\n",
    "PRIMARY_WORKSPACE = os.path.join(ROOT_FOLDER, 'primary')\n",
    "PRIMARY_RES_BLOCKS = 1 # [1,19(AlphaGo Zero),39]\n",
    "PRIMARY_FILTERS = 128 # [128, 192, 256(AlphaGo Zero), 384]\n",
    "PRIMARY_PRE_TRAINED_ITERS = 0\n",
    "# before traning, where to load the params\n",
    "PRIMARY_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-res={}-k={}-iter={}\"\n",
    "                                   .format(PRIMARY_RES_BLOCKS,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS))\n",
    "BASE_LR = -0.01 # (-0.01,0) The base Learning Rate; 0 to disable it.\n",
    "TRAIN_BATCHES = 16 # how many samples will be trained within one mini-batch, depends on your hardware\n",
    "# after training, where to store the params\n",
    "PRIMARY_SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"RL-res={}-k={}-iter={}\"\n",
    "                           .format(PRIMARY_RES_BLOCKS,PRIMARY_FILTERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))\n",
    "if not os.path.exists(PRIMARY_SAVE_FOLDER):\n",
    "    os.makedirs(PRIMARY_SAVE_FOLDER)\n",
    "\n",
    "### Config for sparring partner\n",
    "SPARR_WORKSPACE = os.path.join(ROOT_FOLDER, 'sparring')\n",
    "SPARR_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"res={}-k={}-iter={}\".format(4,128,1))\n",
    "\n",
    "print('Training model from {} to {} iterations'.format(PRIMARY_PRE_TRAINED_ITERS,PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modelingZero import AddResNetModel, AddSoftmax, AddTrainingOperators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the actual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe2.python.predictor.predictor_exporter as pe\n",
    "\n",
    "data = np.empty(shape=(TRAIN_BATCHES,17,19,19), dtype=np.float32)\n",
    "expect = np.empty(shape=(TRAIN_BATCHES,362), dtype=np.float32) # expected distribution of probability\n",
    "reward = np.empty(shape=(TRAIN_BATCHES,), dtype=np.float32) # scalar values between [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary player\n",
    ">Train Net: Blob('data','label') ==> Predict Net ==> Loss ==> Backward Propergation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: PadImage.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n",
      "WARNING:root:You are creating an op that the ModelHelper does not recognize: Normalize.\n"
     ]
    }
   ],
   "source": [
    "workspace.SwitchWorkspace(PRIMARY_WORKSPACE, True)\n",
    "\n",
    "with core.DeviceScope(device_opts):\n",
    "    workspace.FeedBlob(\"data\", data)\n",
    "    workspace.FeedBlob('expect', expect)\n",
    "    workspace.FeedBlob('reward', reward)\n",
    "    # for learning from winner\n",
    "    primary_train_model = model_helper.ModelHelper(name=\"primary_train_model\", arg_scope=arg_scope, init_params=True)\n",
    "    predict, value = AddResNetModel(primary_train_model, 'data', num_blocks=PRIMARY_RES_BLOCKS, filters=PRIMARY_FILTERS)\n",
    "    AddTrainingOperators(primary_train_model, predict, None, 'expect', value, 'reward', base_lr=BASE_LR)\n",
    "    workspace.RunNetOnce(primary_train_model.param_init_net)\n",
    "    workspace.CreateNet(primary_train_model.net, overwrite=True)\n",
    "    # \n",
    "    primary_predict_net = pe.prepare_prediction_net(os.path.join(PRIMARY_LOAD_FOLDER, \"policy_model.minidb\"), \"minidb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `LearnFromWinner` takes the result of tournament and train primary player with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LearnFromWinningGames(history, winner, mini_batch=TRAIN_BATCHES):\n",
    "    data = np.empty(shape=(mini_batch,17,19,19), dtype=np.float32)\n",
    "    label = np.empty(shape=(mini_batch,), dtype=np.int32)\n",
    "    #iter = 0\n",
    "    k = 0\n",
    "    for i in range(len(winner)):\n",
    "        #print('Learning {} steps in {} of {} games'.format(iter * TRAIN_BATCHES, i, GAMES_BATCHES))\n",
    "        for step in history[i]:\n",
    "            if (step[0] == 'B' and winner[i] == 'B+') or (step[0] == 'W' and winner[i] == 'W+'):\n",
    "                data[k] = step[2]\n",
    "                label[k] = step[1]\n",
    "                k += 1\n",
    "                #iter += 1\n",
    "                if k == mini_batch:\n",
    "                    k = 0\n",
    "                    workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "                    with core.DeviceScope(device_opts):\n",
    "                        workspace.FeedBlob(\"data\", data)\n",
    "                        workspace.FeedBlob(\"label\", label)\n",
    "                        workspace.RunNet(primary_train_model.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparring partner\n",
    "    Load on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tournament and training\n",
    ">We use a reward function $r(s)$ that is zero for all non-terminal time-steps $t < T$.\n",
    "The outcome $z_t = \\pm r(s_T)$ is the terminal reward at the end of the game from the perspective of the\n",
    "current player at time-step $t$: $+1$ for winning and $-1$ for losing. Weights are then updated at each\n",
    "time-step $t$ by stochastic gradient ascent in the direction that maximizes expected outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from go import GameState, BLACK, WHITE, EMPTY, PASS\n",
    "from preprocessing import Preprocess\n",
    "from game import DEFAULT_FEATURES\n",
    "from datetime import datetime\n",
    "from sgfutil import GetWinner, WriteBackSGF\n",
    "import sgf\n",
    "\n",
    "np.random.seed(datetime.now().microsecond)\n",
    "\n",
    "# construct the model to be exported\n",
    "pe_meta = pe.PredictorExportMeta(\n",
    "    predict_net=primary_predict_net.Proto(),\n",
    "    parameters=[str(b) for b in primary_train_model.params],\n",
    "    inputs=[\"data\"],\n",
    "    outputs=[\"softmax\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tournament in range(PRIMARY_PRE_TRAINED_ITERS, PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS):\n",
    "    # Every 500 tournament, copy current player to opponent. i.e. checkpoint\n",
    "    if tournament > 0 and tournament % 20 == 0:\n",
    "        pe.save_to_db(\"minidb\", os.path.join(PRIMARY_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(PRIMARY_SAVE_FOLDER))\n",
    "        pe.save_to_db(\"minidb\", os.path.join(SPARR_LOAD_FOLDER, \"policy_model_RL_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+tournament)), pe_meta)\n",
    "        print('Checkpoint saved to {}'.format(SPARR_LOAD_FOLDER))\n",
    "    \n",
    "    # Randomly change color of player\n",
    "    PRIMARY_PLAYER = np.random.choice(['B','W'])\n",
    "    if PRIMARY_PLAYER == 'B':\n",
    "        SPARRING_PLAYER = 'W'\n",
    "    else:\n",
    "        SPARRING_PLAYER = 'B'\n",
    "    \n",
    "    # Randomly pickup sparring partner\n",
    "    workspace.SwitchWorkspace(SPARR_WORKSPACE, True)\n",
    "    sparring_param_file = np.random.choice(os.listdir(SPARR_LOAD_FOLDER))\n",
    "    with core.DeviceScope(device_opts):\n",
    "        sparring_predict_net = pe.prepare_prediction_net(os.path.join(SPARR_LOAD_FOLDER, sparring_param_file), \"minidb\")\n",
    "    print('Tournament {} Primary({}) vs Sparring({}|{}) started @{}'\n",
    "          .format(tournament, PRIMARY_PLAYER, SPARRING_PLAYER, sparring_param_file, datetime.now()))\n",
    "\n",
    "    \n",
    "    # Initialize game board and game state\n",
    "    game_state = [ GameState() for i in range(GAMES_BATCHES) ]\n",
    "    game_result = [0] * GAMES_BATCHES # 0 - Not Ended; BLACK - Black Wins; WHITE - White Wins\n",
    "    p = Preprocess(DEFAULT_FEATURES) # Singleton\n",
    "    history = [ [] for i in range(GAMES_BATCHES) ] # history[n][step] stores tuple of (player, x, y, board[n])\n",
    "    board = None # The preprocessed board with shape Nx17x19x19\n",
    "    \n",
    "    # for each step in all games\n",
    "    for step in range(0,722):\n",
    "        \n",
    "        # Preprocess the board\n",
    "        board = np.concatenate([p.state_to_tensor(game_state[i]).astype(np.float32) for i in range(GAMES_BATCHES)])\n",
    "\n",
    "        if step % 2 == 0:\n",
    "            current_player = BLACK\n",
    "            current_color = 'B'\n",
    "        else:\n",
    "            current_player = WHITE\n",
    "            current_color = 'W'\n",
    "\n",
    "        if step % 2 == (PRIMARY_PLAYER == 'W'): # if step %2 == 0 and Primary is Black, or vice versa.\n",
    "            # primary player make move\n",
    "            workspace.SwitchWorkspace(PRIMARY_WORKSPACE)\n",
    "            with core.DeviceScope(device_opts):\n",
    "                workspace.FeedBlob('data', board)\n",
    "                workspace.RunNet(primary_predict_net)\n",
    "        else:\n",
    "            # sparring partner make move\n",
    "            workspace.SwitchWorkspace(SPARR_WORKSPACE)\n",
    "            with core.DeviceScope(device_opts):\n",
    "                workspace.FeedBlob('data', board)\n",
    "                workspace.RunNet(sparring_predict_net)\n",
    "\n",
    "        predict = workspace.FetchBlob('softmax') # [0.01, 0.02, ...] in shape (N,361)\n",
    "\n",
    "        for i in range(GAMES_BATCHES):\n",
    "            if game_result[i]: # game end\n",
    "                continue\n",
    "            else: # game not end\n",
    "                legal_moves = [ x*19+y for (x,y) in game_state[i].get_legal_moves(include_eyes=False)] # [59, 72, ...] in 1D\n",
    "                if len(legal_moves) > 0: # at least 1 legal move\n",
    "                    probabilities = predict[i][legal_moves] # [0.02, 0.01, ...]\n",
    "                    # use numpy.random.choice to randomize the step,\n",
    "                    # otherwise use np.argmax to get best choice\n",
    "                    # current_choice = legal_moves[np.argmax(probabilities)]\n",
    "                    if np.sum(probabilities) > 0:\n",
    "                        current_choice = np.random.choice(legal_moves, 1, p=probabilities/np.sum(probabilities))[0]\n",
    "                    else:\n",
    "                        current_choice = np.random.choice(legal_moves, 1)[0]\n",
    "                    (x, y) = (current_choice/19, current_choice%19)\n",
    "                    history[i].append((current_color, current_choice, board[i]))\n",
    "                    game_state[i].do_move(action = (x, y), color = current_player) # End of Game?\n",
    "                    #print('game({}) step({}) {} move({},{})'.format(i, step, current_color, x, y))\n",
    "                else:\n",
    "                    game_state[i].do_move(action = PASS, color = current_player)\n",
    "                    #print('game({}) step({}) {} PASS'.format(i, step, current_color))\n",
    "                    game_result[i] = game_state[i].is_end_of_game\n",
    "\n",
    "        if np.all(game_result):\n",
    "            break\n",
    "    \n",
    "    # Get the winner\n",
    "    winner = [ GetWinner(game_state[i]) for i in range(GAMES_BATCHES) ] # B+, W+, T\n",
    "    print('Tournament {} Finished with Primary({}) {}:{} Sparring({}) @{}'.\n",
    "          format(tournament, PRIMARY_PLAYER, sum(np.char.count(winner, PRIMARY_PLAYER)),\n",
    "                 sum(np.char.count(winner, SPARRING_PLAYER)), SPARRING_PLAYER, datetime.now()))\n",
    "    \n",
    "    # Save the games(optional)\n",
    "    for i in range(GAMES_BATCHES):\n",
    "        filename = os.path.join(\n",
    "            os.path.expanduser('~'), 'python', 'tutorial_files','selfplay',\n",
    "            '({}_{}_{})vs({})_{}_{}_{}'.format(PRIMARY_CONV_LEVEL, PRIMARY_FILTERS, PRIMARY_PRE_TRAINED_ITERS+tournament,\n",
    "                                            sparring_param_file, i, winner[i],\n",
    "                                            datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")))\n",
    "        WriteBackSGF(winner, history[i], filename)\n",
    "    \n",
    "    # After each tournament, learn from the winner\n",
    "    if BASE_LR != 0:\n",
    "        LearnFromWinningGames(history, winner, mini_batch=TRAIN_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TOURNAMENT_ITERS>0 :\n",
    "    pe.save_to_db(\"minidb\", os.path.join(PRIMARY_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "    print('Results saved to {}'.format(PRIMARY_SAVE_FOLDER))\n",
    "    pe.save_to_db(\"minidb\", os.path.join(SPARR_LOAD_FOLDER, \"policy_model_RL_{}.minidb\".format(PRIMARY_PRE_TRAINED_ITERS+TOURNAMENT_ITERS)), pe_meta)\n",
    "    print('Results saved to {}'.format(SPARR_LOAD_FOLDER))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "315px",
    "width": "367px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "544px",
    "left": "0px",
    "right": "1723px",
    "top": "107px",
    "width": "130px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
