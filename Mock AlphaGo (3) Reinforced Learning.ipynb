{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock AlphaGo (3) - Reinforced Learning\n",
    "In this notebook, we will train the model by letting them compete each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from caffe2.python import core, model_helper, workspace, brew, utils\n",
    "from caffe2.proto import caffe2_pb2\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# how many games will be run in this tournament\n",
    "# if greater than 1, make sure randomize the choice\n",
    "GAMES_ITERS = 16\n",
    "\n",
    "if workspace.has_gpu_support:\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CUDA, workspace.GetDefaultGPUID())\n",
    "    print('Running in GPU mode on default device {}'.format(workspace.GetDefaultGPUID()))\n",
    "else :\n",
    "    device_opts = core.DeviceOption(caffe2_pb2.CPU, 0)\n",
    "    print('Running in CPU mode')\n",
    "\n",
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "    \n",
    "ROOT_FOLDER = os.path.join(os.path.expanduser('~'), 'python', 'tutorial_data','go','param') # folder stores the loss/accuracy log\n",
    "\n",
    "### Config for black player\n",
    "BLACK_WORKSPACE = os.path.join(ROOT_FOLDER,'black')\n",
    "BLACK_CONV_LEVEL = 13\n",
    "BLACK_FILTERS = 192\n",
    "BLACK_PRE_TRAINED_ITERS = 1\n",
    "# before traning, where to load the params\n",
    "BLACK_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-B-conv={}-k={}-iter={}\".format(BLACK_CONV_LEVEL,BLACK_FILTERS,BLACK_PRE_TRAINED_ITERS))\n",
    "\n",
    "### Config for white player\n",
    "WHITE_WORKSPACE = os.path.join(ROOT_FOLDER,'white')\n",
    "WHITE_CONV_LEVEL = 13\n",
    "WHITE_FILTERS = 192\n",
    "WHITE_PRE_TRAINED_ITERS = 1\n",
    "# before traning, where to load the params\n",
    "WHITE_LOAD_FOLDER = os.path.join(ROOT_FOLDER, \"RL-W-conv={}-k={}-iter={}\".format(WHITE_CONV_LEVEL,WHITE_FILTERS,WHITE_PRE_TRAINED_ITERS))\n",
    "\n",
    "# BOARD_POSITION contains SGF symbol which represents each row (or column) of the board\n",
    "# It can be used to convert between 0,1,2,3... and a,b,c,d...\n",
    "# Symbol [tt] or [] represents PASS in SGF, therefore is omitted\n",
    "BOARD_POSITION = 'abcdefghijklmnopqrs'\n",
    "\n",
    "print('Black {}/{}/{} vs. White {}/{}/{}'.format(\n",
    "    BLACK_CONV_LEVEL, BLACK_FILTERS, BLACK_PRE_TRAINED_ITERS,\n",
    "    WHITE_CONV_LEVEL, WHITE_FILTERS, WHITE_PRE_TRAINED_ITERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_LR = -0.003 # (-0.01,0) The base Learning Rate, alphago uses -0.003 and half the number every 80m steps\n",
    "\n",
    "TRAIN_BATCHES = 16 # how many samples will be trained within one mini-batch, depends on your hardware\n",
    "#PRE_TRAINED_ITERS = 60000 # [0, infinity) how many batches the model has been trained before\n",
    "#SKIP_TRAINED_DATA = 0 # [0, infinity) if this is a resumed training, how many input data will be skipped\n",
    "TRAIN_ITERS = 1 # [0, infinity) how many batches the model will be trained\n",
    "\n",
    "# after training, where to store the params\n",
    "BLACK_SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"RL-B-conv={}-k={}-iter={}\".format(BLACK_CONV_LEVEL,BLACK_FILTERS,BLACK_PRE_TRAINED_ITERS+TRAIN_ITERS))\n",
    "\n",
    "# after training, where to store the params\n",
    "WHITE_SAVE_FOLDER = os.path.join(ROOT_FOLDER, \"RL-W-conv={}-k={}-iter={}\".format(WHITE_CONV_LEVEL,WHITE_FILTERS,WHITE_PRE_TRAINED_ITERS+TRAIN_ITERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Neural Network Architecture\n",
    "Refer to AlphaGo\n",
    ">  We also trained a faster but less accurate rollout policy pπ(a|s), using a linear softmax of small pattern features (see Extended Data Table 4) with weights π; this achieved an accuracy of 24.2%, using just 2µs to select an action, rather than 3ms for the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modeling import AddConvModel, AddTrainingOperators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the actual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe2.python.predictor.predictor_exporter as pe\n",
    "\n",
    "data = np.empty(shape=(TRAIN_BATCHES,48,19,19), dtype=np.float32)\n",
    "label = np.empty(shape=(TRAIN_BATCHES,1), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Net Initialize\n",
    ">Train Net: Blob('data','label') ==> Predict Net ==> Loss ==> Backward Propergation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Black player\n",
    "workspace.SwitchWorkspace(BLACK_WORKSPACE, True)\n",
    "with core.DeviceScope(device_opts):\n",
    "    black_train_model = model_helper.ModelHelper(name=\"black_train_model\", arg_scope=arg_scope, init_params=True)\n",
    "    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "    predict = AddConvModel(black_train_model, \"data\", conv_level=BLACK_CONV_LEVEL, filters=BLACK_FILTERS)\n",
    "    workspace.FeedBlob(\"label\", data, device_option=device_opts)\n",
    "    AddTrainingOperators(black_train_model, \"predict\", \"label\", base_lr=BASE_LR)\n",
    "    workspace.RunNetOnce(black_train_model.param_init_net)\n",
    "    workspace.CreateNet(black_train_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize White player\n",
    "workspace.SwitchWorkspace(WHITE_WORKSPACE, True)\n",
    "with core.DeviceScope(device_opts):\n",
    "    white_train_model = model_helper.ModelHelper(name=\"white_train_model\", arg_scope=arg_scope, init_params=True)\n",
    "    workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "    predict = AddConvModel(white_train_model, \"data\", conv_level=WHITE_CONV_LEVEL, filters=WHITE_FILTERS)\n",
    "    workspace.FeedBlob(\"label\", data, device_option=device_opts)\n",
    "    AddTrainingOperators(white_train_model, \"predict\", \"label\", base_lr=BASE_LR)\n",
    "    workspace.RunNetOnce(white_train_model.param_init_net)\n",
    "    workspace.CreateNet(white_train_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Net\n",
    "Build Deploy Net and Init Deploy Net with saved weight and bias.\n",
    ">Predict Net: Blob('data') ==> Predict Net ==> Blob('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Black player\n",
    "workspace.SwitchWorkspace(BLACK_WORKSPACE, True)\n",
    "with core.DeviceScope(device_opts):\n",
    "    black_deploy_model = model_helper.ModelHelper(name=\"black_policy_deploy\", arg_scope=arg_scope, init_params=False)\n",
    "    if BLACK_PRE_TRAINED_ITERS > 0:\n",
    "        black_deploy_model.net = pe.prepare_prediction_net(os.path.join(BLACK_LOAD_FOLDER, \"policy_model.minidb\"), \"minidb\", device_option=device_opts)\n",
    "    else:\n",
    "        AddConvModel(black_deploy_model, \"data\", conv_level=BLACK_CONV_LEVEL, filters=BLACK_FILTERS)\n",
    "        workspace.RunNetOnce(black_deploy_model.param_init_net)\n",
    "        workspace.CreateNet(black_deploy_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize White player\n",
    "workspace.SwitchWorkspace(WHITE_WORKSPACE, True)\n",
    "with core.DeviceScope(device_opts):\n",
    "    white_deploy_model = model_helper.ModelHelper(name=\"white_policy_deploy\", arg_scope=arg_scope, init_params=False)\n",
    "    if WHITE_PRE_TRAINED_ITERS > 0:\n",
    "        white_deploy_model.net = pe.prepare_prediction_net(os.path.join(WHITE_LOAD_FOLDER, \"policy_model.minidb\"), \"minidb\", device_option=device_opts)\n",
    "    else:\n",
    "        AddConvModel(white_deploy_model, \"data\", conv_level=WHITE_CONV_LEVEL, filters=WHITE_FILTERS)\n",
    "        workspace.RunNetOnce(white_deploy_model.param_init_net)\n",
    "        workspace.CreateNet(white_deploy_model.net, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tournament and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from go import GameState, BLACK, WHITE, EMPTY\n",
    "from preprocessing import Preprocess\n",
    "\n",
    "DEFAULT_FEATURES = [\n",
    "    \"board\", \"ones\", \"turns_since\", \"liberties\", \"capture_size\",\n",
    "    \"self_atari_size\", \"liberties_after\", \"ladder_capture\", \"ladder_escape\",\n",
    "    \"sensibleness\", \"zeros\"]\n",
    "\n",
    "game_state = [GameState() for i in range(GAMES_ITERS)]\n",
    "game_result = [0] * GAMES_ITERS # 0 - Not Ended; BLACK - Black Wins; WHITE - White Wins\n",
    "p = [Preprocess(DEFAULT_FEATURES)] * GAMES_ITERS\n",
    "history = [[] for i in range(GAMES_ITERS)]\n",
    "# board before current move\n",
    "board = [p[i].state_to_tensor(game_state[i]).astype(np.float32) for i in range(GAMES_ITERS)]\n",
    "\n",
    "# diverse the first step for GAMES_ITERS choices\n",
    "workspace.SwitchWorkspace(BLACK_WORKSPACE)\n",
    "workspace.FeedBlob('data', board[0], device_option=device_opts)\n",
    "\n",
    "workspace.RunNet(black_deploy_model.net)\n",
    "init_move = np.reshape(workspace.FetchBlob('predict'), (-1))\n",
    "init_sorted_move = np.argsort(-init_move) # shape=(361,)\n",
    "\n",
    "current_choice = init_sorted_move[0:GAMES_ITERS]\n",
    "x = current_choice/19 # tensor\n",
    "y = current_choice%19 # tensor\n",
    "\n",
    "for i in range(GAMES_ITERS):\n",
    "    history[i].append(('B',x[i],y[i],board[i]))\n",
    "    game_state[i].do_move(action=(x[i],y[i]),color = BLACK)\n",
    "    print('game({}) step({}) black move({},{})'.format(i, 0, x[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each step in the game\n",
    "for step in range(1,500):\n",
    "\n",
    "#    for i in range(GAMES_ITERS):\n",
    "#        if not game_result[i]:\n",
    "#            board[i] = np.append(board[i], p[i].state_to_tensor(game_state[i]).astype(np.float32), axis=0)\n",
    "    board = np.concatenate([p[i].state_to_tensor(game_state[i]).astype(np.float32) for i in range(GAMES_ITERS)])\n",
    "\n",
    "    if step % 2 == 0:\n",
    "        # black move\n",
    "        workspace.SwitchWorkspace(BLACK_WORKSPACE)\n",
    "        workspace.FeedBlob('data', board, device_option=device_opts)\n",
    "        workspace.RunNet(black_deploy_model.net)\n",
    "        move = np.reshape(workspace.FetchBlob('predict'), (GAMES_ITERS,-1))\n",
    "        sorted_move = np.argsort(-move)\n",
    "        for i in range(GAMES_ITERS):\n",
    "            if game_result[i]:\n",
    "                break;\n",
    "            legal_moves = [ x*19+y for (x,y) in game_state[i].get_legal_moves(include_eyes=False)] # [59, 72, ...]\n",
    "            if len(legal_moves) > 0: # at least 1 legal move\n",
    "                mask = np.in1d(sorted_move[i], legal_moves) # [True, False, True, ...]\n",
    "                current_choice = sorted_move[i][mask][0] # The top legal move\n",
    "                (x, y) = (current_choice/19, current_choice%19)\n",
    "                history[i].append(('B',x,y,board[i]))\n",
    "                game_state[i].do_move(action=(x,y),color = BLACK) # End of Game?\n",
    "                print('game({}) step({}) black move({},{})'.format(i, step, x, y))\n",
    "            else:\n",
    "                game_result[i] = game_state[i].is_end_of_game\n",
    "    else:\n",
    "        # white move\n",
    "        workspace.SwitchWorkspace(WHITE_WORKSPACE)\n",
    "        workspace.FeedBlob('data', board, device_option=device_opts)\n",
    "        workspace.RunNet(white_deploy_model.net)\n",
    "        move = np.reshape(workspace.FetchBlob('predict'), (GAMES_ITERS,-1))\n",
    "        sorted_move = np.argsort(-move)\n",
    "        for i in range(GAMES_ITERS):\n",
    "            if game_result[i]:\n",
    "                break;\n",
    "            legal_moves = [ x*19+y for (x,y) in game_state[i].get_legal_moves(include_eyes=False)] # [59, 72, ...]\n",
    "            if len(legal_moves) > 0: # at least 1 legal move\n",
    "                mask = np.in1d(sorted_move[i], legal_moves) # [True, False, True, ...]\n",
    "                current_choice = sorted_move[i][mask][0] # The top legal move\n",
    "                (x, y) = (current_choice/19, current_choice%19)\n",
    "                history[i].append(('W',x,y,board[i]))\n",
    "                game_state[i].do_move(action=(x,y),color = WHITE) # End of Game?\n",
    "                print('game({}) step({}) white move({},{})'.format(i, step, x, y))\n",
    "            else:\n",
    "                game_result[i] = game_state[i].is_end_of_game\n",
    "\n",
    "    if np.all(game_result):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record the game in SGF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgfutil import WriteBackSGF\n",
    "\n",
    "#comment out for better performance\n",
    "for i in range(GAMES_ITERS):\n",
    "    filename = os.path.join(\n",
    "        os.path.expanduser('~'), 'python', 'tutorial_files','selfplay',\n",
    "        '({}_{}_{})vs({}_{}_{})_{}'.format(BLACK_CONV_LEVEL, BLACK_FILTERS, BLACK_PRE_TRAINED_ITERS,\n",
    "                                        WHITE_CONV_LEVEL, WHITE_FILTERS, WHITE_PRE_TRAINED_ITERS, i))\n",
    "    WriteBackSGF(game_state[i], history[i], filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn from the winning games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = 0\n",
    "k = 0\n",
    "for i in range(GAMES_ITERS):\n",
    "    winner = game_state[i].get_winner()\n",
    "    print('Learning {} steps in {} of {} games. {} wins'.format(iter * 32, i, GAMES_ITERS, history[i][0][0]))\n",
    "    for step in history[i]:\n",
    "        if (step[0] == 'B' and winner == BLACK) or (step[0] == 'W' and winner == WHITE):\n",
    "            data[k] = step[3]\n",
    "            label[k] = step[1]*19+step[2]\n",
    "            k += 1\n",
    "            if k == TRAIN_BATCHES:\n",
    "                iter += 1\n",
    "                k = 0\n",
    "                workspace.SwitchWorkspace(BLACK_WORKSPACE)\n",
    "                workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "                workspace.FeedBlob(\"label\", label, device_option=device_opts)\n",
    "                workspace.RunNet(black_train_model.net)\n",
    "                workspace.SwitchWorkspace(WHITE_WORKSPACE)\n",
    "                workspace.FeedBlob(\"data\", data, device_option=device_opts)\n",
    "                workspace.FeedBlob(\"label\", label, device_option=device_opts)\n",
    "                workspace.RunNet(white_train_model.net)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(BLACK_SAVE_FOLDER):\n",
    "    os.makedirs(BLACK_SAVE_FOLDER)\n",
    "# construct the model to be exported\n",
    "pe_meta = pe.PredictorExportMeta(\n",
    "    predict_net=black_deploy_model.net.Proto(),\n",
    "    parameters=[str(b) for b in black_deploy_model.params], \n",
    "    inputs=[\"data\"],\n",
    "    outputs=[\"predict\"],\n",
    ")\n",
    "pe.save_to_db(\"minidb\", os.path.join(BLACK_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "print('Params saved to {}'.format(BLACK_SAVE_FOLDER))\n",
    "    \n",
    "if not os.path.exists(WHITE_SAVE_FOLDER):\n",
    "    os.makedirs(WHITE_SAVE_FOLDER)\n",
    "# construct the model to be exported\n",
    "pe_meta = pe.PredictorExportMeta(\n",
    "    predict_net=white_deploy_model.net.Proto(),\n",
    "    parameters=[str(b) for b in white_deploy_model.params], \n",
    "    inputs=[\"data\"],\n",
    "    outputs=[\"predict\"],\n",
    ")\n",
    "pe.save_to_db(\"minidb\", os.path.join(WHITE_SAVE_FOLDER, \"policy_model.minidb\"), pe_meta)\n",
    "print('Params saved to {}'.format(WHITE_SAVE_FOLDER))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "315px",
    "width": "367px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "544px",
    "left": "0px",
    "right": "1723px",
    "top": "107px",
    "width": "130px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
